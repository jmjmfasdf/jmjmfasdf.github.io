---
title: "Ten simple rules for the computational modeling of behavioral data"
date: 2025-03-12
tags:
    - Modeling
    - fMRI
    - EEG
    - Behavioral Data
categories: 
    - Paper Review
toc: true
toc_sticky: true
---

컴퓨테이셔널 모델링(computational modeling)은 심리학과 신경과학 연구에서 중요한 혁신을 가져왔다. 실험 데이터를 모델에 적합하게 피팅(fitting)하면 행동의 근본적인 알고리즘을 탐구하고, 계산적 변수의 신경학적 상관(neural correlates)을 찾을 수 있으며, 약물, 질병, 중재(intervention)의 영향을 더 깊이 이해할 수 있다. 본 논문에서는 컴퓨테이셔널 모델링을 신중하게 사용하고 의미 있는 통찰을 얻기 위한 10가지 간단한 규칙을 제시한다. 특히, 초보 연구자들이 쉽게 접근할 수 있도록 모델과 데이터를 연결하는 방법을 실용적이고 세부적인 관점에서 설명하고 있다.

**"모델이 우리에게 정신(mind)에 대해 정확히 무엇을 말해줄 수 있는가?"**라는 핵심 질문을 다루기 위해, 저자들은 가장 기본적인 모델링 기법을 중심으로 설명하며, 실험 코드와 예제를 통해 개념을 구체적으로 보여준다. 그러나 이 논문에서 제시하는 대부분의 규칙은 더 발전된 모델링 기법에도 적용할 수 있다.

# 1. What is computational modeling of behavioral data?

행동 데이터의 컴퓨테이셔널 모델링(computational modeling of behavioral data)이란 수학적 모델을 이용해 행동 데이터를 보다 체계적으로 이해하는 방법을 의미한다. 행동 데이터는 주로 **선택(choice)** 형태로 나타나지만, **반응 시간(reaction time), 시선 움직임(eye movement), 신경 활동(neural data)** 등의 다양한 관찰 가능한 변수들도 포함될 수 있다. 이 모델들은 실험에서 관찰할 수 있는 자극(stimuli), 결과(outcomes), 과거 경험(past experiences) 등의 변수와 미래 행동을 수학적으로 연결하는 방정식의 형태를 갖는다. 즉, 컴퓨테이셔널 모델은 행동이 생성되는 과정에 대한 ‘알고리즘적 가설(algorithmic hypothesis)’을 구체적으로 구현하는 역할을 한다.

행동 데이터를 '이해한다'는 의미는 연구자의 목표에 따라 다를 수 있다. 어떤 경우에는 데이터를 설명할 수 있는 단순한 모델로도 충분하지만, 보다 정량적인 예측을 제공하는 복잡한 모델이 필요한 경우도 있다. 연구자들이 컴퓨테이셔널 모델을 사용하는 주요 목적은 크게 네 가지로 나눌 수 있다.

**시뮬레이션(Simulation)**

모델의 특정한 매개변수(parameter) 설정하에 가상의 ‘행동 데이터’를 생성하는 과정이다. 이러한 시뮬레이션 데이터를 실제 데이터처럼 분석하면, 행동의 질적 및 양적 패턴을 예측하고 검증할 수 있다. 즉, 시뮬레이션을 통해 이론적 예측을 보다 정밀하게 만들고 실험적으로 검증할 수 있도록 한다.

**매개변수 추정(Parameter Estimation)**

주어진 모델이 실험 데이터를 가장 잘 설명할 수 있도록 매개변수 값을 찾는 과정이다. 이렇게 얻어진 매개변수는 데이터의 요약 정보로 활용될 수 있으며, 개인차(individual differences)를 연구하거나 약물, 병리적 상태, 실험적 조작(intervention) 등의 영향을 정량화하는 데 사용될 수 있다.

**모델 비교(Model Comparison)**

여러 개의 가능한 모델 중 어떤 모델이 행동 데이터를 가장 잘 설명하는지를 평가하는 과정이다. 특히, 서로 유사한 질적 예측을 하지만 정량적 차이가 있는 모델들을 비교할 때 유용하다. 모델 비교를 통해 행동을 생성하는 기저 메커니즘을 더 잘 이해할 수 있다.

**잠재 변수 추론(Latent Variable Inference)**

행동 데이터에서 직접 관찰할 수 없는 숨겨진 변수(latent variable)의 값을 추정하는 과정이다. 예를 들어, 어떤 선택이 주어진 상황에서 얼마나 가치가 있는지(value of choices)와 같은 정보는 직접 측정할 수 없지만, 모델을 통해 추론할 수 있다. 이러한 기법은 특히 신경영상(neuroimaging) 연구에서 활용되며, EEG, ECOG, 전기생리학(electrophysiology), 동공 측정(pupillometry) 등의 다양한 데이터와 결합되어 신경 메커니즘을 밝히는 데 기여한다.

<br>

컴퓨테이셔널 모델링은 강력한 도구이지만, 잘못 사용될 경우 잘못된 결론을 도출할 위험이 있다. 시뮬레이션, 매개변수 추정, 모델 비교, 잠재 변수 추론 각각은 특정한 강점과 약점을 가지고 있으며, 부주의하게 다루면 오해를 유발할 수 있다. 따라서 논문에서는 초보자도 이해할 수 있도록 실용적이고 세부적인 접근 방식을 제시하고, 모델을 데이터와 어떻게 연결해야 하는지, 그리고 모델링 과정에서 발생할 수 있는 일반적인 실수를 어떻게 피할 수 있는지를 설명하고자 한다.

이 논문의 목표는 단순히 모델을 구현하는 기술적 측면을 다루는 것이 아니다. 대신, 모델이 인간의 인지 과정과 행동을 어떻게 설명하는지를 보다 심층적으로 탐구하는 데 중점을 둔다. 이를 위해 가장 기초적인 모델링 기법을 중심으로 설명하지만, 논문에서 제시하는 원칙들은 보다 복잡한 모델에도 적용될 수 있다. 또한, 보다 심화된 모델링 기법을 다루는 다양한 튜토리얼, 예제, 그리고 교재들을 참고할 것을 권장한다.

논문에서는 설명의 명확성을 위해 강화학습(reinforcement learning) 모델을 선택하고, 이를 선택 행동(choice data)에 적용하는 예제를 중심으로 설명한다. 이 특정한 도메인을 선택한 이유는 다음과 같다.

**1. 강화학습 모델은 학습 과정에 대한 연구에서 특히 인기가 많다.**

행동 데이터에서 학습의 특성을 분석할 때, 강화학습 모델은 중요한 도구가 될 수 있다. 특히, 행동 데이터에서 개별적인 시도(trial)는 모든 과거 경험에 영향을 받으며, 이는 고전적인 조건별 데이터 분석(aggregation across conditions)을 어렵게 만든다. 따라서, 컴퓨테이셔널 모델링이 이러한 데이터의 특성을 포착하는 데 유리하다.

**2. 학습 과정에서의 연속적 의존성(sequential dependency)은 모델 피팅(fitting) 과정에서 독특한 기술적 문제를 유발한다.**

비학습(non-learning) 과제에서는 존재하지 않는 이러한 문제를 다루는 방법을 학습할 필요가 있다.

이 논문에서 다루는 모델링 기법들은 강화학습뿐만 아니라 다른 행동 데이터에도 광범위하게 적용될 수 있다. 예를 들어,

- 반응 시간(reaction time) 모델링 (Ratcliff & Rouder, 1998; Viejo et al., 2015)  
- 지각(perception) 및 지각적 의사결정(perceptual decision-making) (Sims, 2018; Drugowitsch et al., 2016)  
- 경제적 의사결정(economic decision-making) (van Ravenzwaaij et al., 2011; Nilsson et al., 2011)  
- 단기 기억(visual short-term memory) (Donkin et al., 2016)  
- 장기 기억(long-term memory) (Batchelder & Riefer, 1990)  
- 범주 학습(category learning) (Lee & Webb, 2005)  
- 집행 기능(executive functions) (Haaf & Rouder, 2017)  

등 다양한 분야에서도 동일한 모델링 원칙이 적용될 수 있다.

<br>

# 2. Design a good experiment!

컴퓨테이셔널 모델링은 강력한 도구이지만, 좋은 실험 설계를 대체할 수는 없다. 모델링은 관찰된 행동을 설명하는 기저 메커니즘을 포착하는 것이 목표이지만, 기본적으로 행동 데이터에 의존하며, 행동 데이터는 실험 설계에 의해 결정된다. 예를 들어, 얼굴 인식(face perception)을 연구하는 사람이 Prospect Theory(전망 이론)를 적용할 수 없고, 손실과 이익의 차이를 연구하려는 연구자가 이익만 제공하는 도박(task with only gains) 실험을 설계하는 것은 의미가 없다. 이러한 단순한 예시에서는 당연해 보이지만, 연구의 복잡성이 증가할수록 적절한 실험 설계를 보장하는 것이 어려워진다.

예를 들어, 특정한 학습 프로토콜이 학습률의 동적 변화(dynamic changes in learning rate), 작업 기억(working memory) 또는 일화 기억(episodic memory)의 기여도를 식별할 수 있을 정도로 충분한 정보를 제공하는가? 보상의 범위(reward range adaptation)를 반영할 수 있는가? 이러한 질문에 대한 답은 실험 프로토콜이 신중하게 설계되지 않는 한 대부분 "아니오"일 것이다.

그렇다면, 컴퓨테이셔널 모델링을 염두에 두고 좋은 실험을 설계하려면 어떻게 해야 할까? 이 과정은 어느 정도 예술적인 감각을 필요로 하지만, 최적의 실험 설계를 위해 고려해야 할 몇 가지 핵심 질문이 있다.

## 1. 당신이 연구하려는 과학적 질문은 무엇인가?

이 질문은 매우 기본적이지만, 연구를 진행하다 보면 이를 명확하게 정의하지 않고 실험 설계를 하게 되는 경우가 많다.

1. 연구하고자 하는 인지 과정(cognitive process) 은 무엇인가?  
2. 어떤 행동적 특성(behavioral aspect) 을 포착하고 싶은가?  
3. 어떤 가설(hypothesis) 을 검증하거나 비교하려 하는가?  
4. 예를 들어, 실험을 통해 작업 기억이 학습에 어떻게 기여하는지 혹은 행동의 가변성(behavioral variability)이 탐색(exploration)에 어떻게 영향을 미치는지 를 밝히려 한다면, 실험 설계에서 이러한 과정이 드러나도록 해야 한다. 연구 목표를 명확히 설정하면 이후 분석 과정에서 시간을 절약할 수 있다.

## 2. 실험이 목표로 하는 인지 과정을 실제로 활성화하는가?

이 질문은 단순하지 않으며, 전문가의 의견이나 파일럿 실험(piloting)이 필요할 수도 있다.

1. 실험 설계가 연구하고자 하는 과정(processes) 을 실제로 유발하는가?  
2. 단순한 행동 데이터만 수집하는 것이 아니라, 인지적 기전(cognitive mechanisms) 이 반영될 수 있는가?  
예를 들어, 특정한 학습 전략을 연구하고 싶다면, 학습이 일어나는 과정이 실험에서 명확하게 나타나야 한다. 단순한 자극-반응(stimulus-response) 실험이 아니라, 피험자가 전략적으로 학습을 해야 하는 상황이 주어져야 한다.

## 3. 단순한 데이터 통계(statistics)만으로도 목표로 하는 과정이 확인되는가?

좋은 실험은 컴퓨테이셔널 모델링을 수행하기 전에 이미 특정한 효과를 확인할 수 있어야 한다 (Palminteri et al., 2017). 즉, 모델링 없이도 행동 데이터의 단순한 통계 분석을 통해 연구하고자 하는 과정이 드러나는 것이 이상적이다. 예를 들어, 작업 기억(working memory)의 역할을 연구하려면, 실험 설계에서 기억 부하(memory load)가 변화할 수 있도록 조정해야 한다. 보상 감수성(reward sensitivity)을 연구하려면, 보상의 크기나 빈도에 따라 행동 패턴이 달라지는지를 확인해야 한다. 이렇게 하면, 단순한 행동 분석에서도 인지 과정의 일부가 드러나며, 모델링을 통해 보다 정밀한 분석을 수행할 수 있다. 만약 행동 데이터에서 특정한 효과가 보이지 않는다면, 모델링을 수행하더라도 유의미한 결과를 얻기 어렵다.

## 4. 실험 설계와 모델 설계는 동시에 이루어져야 한다.

좋은 실험을 설계하는 것은 좋은 모델을 설계하는 과정과 밀접하게 연결되어 있어야 한다. 실험이 특정한 인지 과정과 행동 패턴을 포착할 수 있어야 하고, 모델이 그러한 데이터를 적절히 설명할 수 있어야 한다. 실험 설계를 먼저 완성한 후에 모델을 적용하는 것이 아니라, 초기 단계에서부터 실험과 모델을 함께 설계하는 것이 이상적이다.

## 5. "나는 실험을 직접 수행하지 않는 모델러인데, 이 과정을 무시해도 될까?"

컴퓨테이셔널 모델링 연구자 중에는 실험을 직접 수행하지 않고, 기존의 출판된 연구 결과나 공개 데이터셋(public dataset)을 분석하는 경우도 많다. 이러한 모델러들은 실험 설계에 대해 신경 쓰지 않아도 된다고 생각할 수도 있다. 그러나 저자들은 이에 동의하지 않는다. 모델러들도 자신의 모델을 검증할 수 있는 더 나은 실험이 가능할지 고민해야 하고, 모델이 적용될 수 있는 실제 행동 데이터가 어떤 특성을 가지는지 파악해야 한다. 이러한 사고 방식은 모델을 보다 구체적이고 실질적으로 만들며, 모델이 적용될 수 있는 범위를 넓혀줄 수 있다. 특히, 실험을 직접 수행하는 연구자들과 협력하면, 모델이 현실적인 데이터에 더 적합하게 조정될 수 있으며, 새로운 통찰을 얻을 수 있다.

마지막으로, 실험 설계에 대한 구체적인 아이디어를 제안하면, 실험을 수행하는 연구자들이 실제로 그 실험을 실행하도록 설득할 수 있는 가능성도 높아진다. 연구를 발전시키기 위해서는 모델러와 실험 연구자 간의 협력이 필수적이다.

<br>

# An illustrative example: the multi-armed bandit task

논문에서 제시하는 10가지 모델링 규칙은 매우 일반적이지만, 이를 보다 구체적으로 설명하기 위해 강화학습(reinforcement learning) 모델을 활용한 선택 과제(choice task) 를 예시로 들고 있다. 이 논문에서 다루는 예제들은 사람들이 어떻게 보상을 극대화하는지를 학습하는 과정을 분석하는 것이 목표이다. 특히, 최적의 선택이 처음에는 알려져 있지 않은 상황에서 학습이 어떻게 진행되는지를 연구한다.

## 멀티 암드 밴딧(Multi-Armed Bandit) 과제란?

이 실험은 여러 개의 슬롯머신(slot machine, 또는 "one-armed bandit") 중 하나를 선택하는 방식으로 진행된다. 실험 참가자는 T번의 선택(trials) 을 수행하며, K개의 슬롯머신 중 하나를 선택해야 한다. 각 슬롯머신 $$k$$ 는 선택(trial $$t$$)될 때, 특정한 확률 $$θ_k$$ 로 보상(reward, $$r_t$$)을 지급한다.

즉, 슬롯머신 $$k$$ 를 선택했을 때, 보상 $$r_t$$ 을 받을 확률은 $$θ_k$$ 이며, $$θ_k$$ 는 슬롯머신마다 다르며, 실험 참가자는 이를 사전에 알지 못한다(unknown to the subject). 가장 단순한 실험에서는 보상 확률 $$θ_k$$ 가 시간에 따라 변하지 않고 고정(fixed)되어 있다.

## 멀티 암드 밴딧 과제의 실험 변수

이 실험에서는 다음과 같은 3가지 주요 변수를 설정할 수 있다.

- 시행 횟수 ($$T$$): 참가자가 선택을 몇 번 반복하는가?  
- 슬롯머신 개수 ($$K$$): 몇 개의 슬롯머신이 있는가?  
- 보상 확률 ($$θ_k$$): 각 슬롯머신이 보상을 줄 확률이 얼마인가?

이러한 변수 설정이 실험의 결과에 중요한 영향을 미치며, 논문에서 사용한 기본 실험 설정은 다음과 같다.

- 시행 횟수: $$T = 1000$$
- 슬롯머신 개수: $$K = 2$$
- 슬롯머신의 보상 확률:
- 슬롯머신 1: $$θ_1 = 0.2$$ (20% 확률로 보상)
- 슬롯머신 2: $$θ_2 = 0.8$$ (80% 확률로 보상)

즉, 한 참가자는 두 개의 슬롯머신 중 하나를 선택해야 하며, 각각의 슬롯머신이 보상을 줄 확률은 미리 정해져 있지만 참가자는 이를 모른다. 실험 참가자는 반복적인 선택을 통해 어느 슬롯머신이 더 높은 보상을 주는지 학습해야 한다.

## 이 과제가 왜 중요한가?

멀티 암드 밴딧 과제는 탐색-활용(exploration-exploitation) 문제를 연구하는 데 매우 유용하다. 탐색(Exploration)은 더 나은 선택을 하기 위해 미지의 선택지를 시도하는 것이며, 활용(Exploitation)은 현재까지 학습한 정보를 바탕으로 가장 보상이 높을 것으로 예상되는 선택지를 선택하는 것이라 할 수 있다.

예를 들어, 초반에는 두 슬롯머신의 보상 확률을 모른 상태이므로 여러 번 선택을 시도하며 탐색(exploration) 을 수행해야 한다. 실험이 진행됨에 따라 더 높은 확률로 보상을 주는 슬롯머신(슬롯머신 2, $$θ_2$$ = 0.8)을 인식하고 이를 점점 더 많이 선택하는 전략(활용, exploitation) 을 사용할 것이다. 이 과제는 강화학습 모델을 테스트하는 데 매우 적합하며, 다양한 학습 알고리즘을 비교하는 데 사용할 수 있다.

<br>

# Design good models

## 1. 모델링의 목적을 명확히 하라

모델을 설계할 때 가장 중요한 것은 모델을 사용하는 이유를 명확히 하는 것이다.

1. 설명적 모델(Descriptive model)

행동 데이터를 요약하는 것이 목적
예: 반응시간 분포를 단순한 수학적 공식으로 요약하는 모델

2. 기계론적 모델(Mechanistic model)

행동과 뇌의 메커니즘을 연결하는 것이 목표
예: 신경과학에서 보상 학습과 도파민 신호를 연결하는 강화학습 모델

3. 개념적 모델(Conceptual model)

특정 이론적 개념을 수학적으로 표현하는 것이 목적
예: 강화학습 이론에서 탐색(exploration)과 활용(exploitation) 간 균형을 설명하는 모델

Kording et al. (2018)에 따르면, 컴퓨테이셔널 모델링 연구자들은 매우 다양한 목표를 가지고 있으며, 자신이 어떤 목표를 가지고 모델링을 수행하는지 명확히 아는 것이 가장 중요하다.

## 2. 모델을 설계하는 다양한 접근법

컴퓨테이셔널 모델을 설계하는 방법은 여러 가지가 있으며, 연구 목적에 따라 적절한 방법을 선택할 수 있다.

1) 휴리스틱 기반 접근법 (Heuristic Approach)

단순한 규칙을 사용하여 특정 행동을 설명하는 모델을 설계하는 방법이며, 경험을 통해 행동이 어떻게 변화하는지 간단한 방식으로 설명한다.

예시: 델타 학습 규칙(Delta rule)

$$
ΔV = α (R - V)
$$

Rescorla-Wagner 모델(Rescorla & Wagner, 1972)에서 사용되었다.

2) 인공지능 및 수학적 알고리즘을 참고하는 접근법

인공지능(AI), 컴퓨터 과학, 응용 수학 분야에서 사용되는 알고리즘을 참고하여 인간의 행동을 설명하는 모델을 구축하며, 인간과 동물의 행동 및 신경 기제를 설명하는 데 사용되었다.

예시: 강화학습 모델(Reinforcement Learning) - Q-learning (Watkins & Dayan, 1992), Temporal Difference Learning (Sutton & Barto, 2018)

3) 베이즈 최적 모델 (Bayes-optimal Models)

환경과 과제에 대한 최적의 해결책을 찾는 모델을 설계하며, 인지 과정이 최적의 방식으로 작동하는지를 테스트하는 데 유용하다. 보다 현실적인 모델을 만들기 위해, 제약 조건(bounded rationality constraints) 을 추가할 수도 있다.

예시: 이상적 관찰자 모델(Ideal Observer Model)
시각 인지 연구에서 자극을 최적적으로 처리하는 방법을 설명하는 모델 (Geisler, 2011)
계산 자원의 한계를 고려하는 모델 (Lieder et al., 2018)

## 3. 좋은 모델을 만들기 위해 지켜야 할 원칙

모델을 설계할 때 다음 세 가지 원칙을 따라야 한다.

1. 모델은 최대한 단순해야 하지만, 너무 단순해서는 안 된다.

아인슈타인의 명언:  
**"Everything should be made as simple as possible, but not simpler."**

단순한 모델은 적합(fitting)이 쉽고 해석이 용이하다. 과도하게 복잡한 모델은 데이터를 잘 설명하는 것처럼 보일 수 있지만, 실제로는 과적합(overfitting) 문제를 초래할 가능성이 크다. 모델 비교 기법(model comparison techniques) 은 과적합을 방지하기 위해 과도하게 복잡한 모델에 패널티를 부여한다(자세한 내용은 Appendix 2 참고).

2. 모델은 해석 가능해야 한다.

모델이 행동 데이터를 잘 설명한다고 하더라도, 해석할 수 없는 모델은 유용하지 않다. 예를 들어서, 강화학습 모델에서 음의 학습률(negative learning rate) 은 논리적으로 해석하기 어렵다. 의미 없는 매개변수가 모델에 포함될 경우, 이는 단순히 모델이 중요한 요소를 놓치고 있음을 나타낼 수도 있다. 따라서 모델의 각 요소가 의미 있는 방식으로 행동 데이터를 설명할 수 있어야 한다.

3. 모델은 연구자가 테스트하고자 하는 모든 가설을 포착할 수 있어야 한다.

특정한 가설만 고려하는 모델을 만들면, 모델 비교(model comparison)를 수행할 때 제한적일 수 있다. 덧붙여서, 가설을 검증하는 데 사용되는 모델만이 아니라, 경쟁 가설(competing hypotheses)을 반영할 수 있는 모델도 포함해야 한다. 예를 들어서 기본 모델이 랜덤한 행동을 하는 모델이고, 가설 모델 (Hypothesis Model)이 연구자가 검증하려는 학습 모델이라면 학습이 아닌 다른 메커니즘으로 행동을 설명하는 대조 모델 (Competing Model)이 있으면 좋다는 것이다. 연구자가 선호하는 특정 모델에 너무 의존하지 않고, 데이터가 가장 적절한 모델을 선택하도록 해야 한다.

<br>

# Example: Modeling behavior in the multi-armed  bandit task.

이 논문에서는 Multi-Armed Bandit Task에서 사람들이 어떻게 행동하는지를 설명하는 다섯 가지 모델을 제시하고 있다. 각 모델의 핵심 개념과 수식을 설명하면 다음과 같다.

## Model 1: Random Responding (랜덤 응답 모델)

이 모델은 참가자가 과제에 전혀 몰입하지 않고 단순히 무작위로 버튼을 누르는 경우를 가정한다. 그러나 참가자가 특정 선택지에 대한 선호도(bias)를 가질 수도 있다고 본다. 이 선호도는 파라미터 $$ b $$ 로 표현되며, 다음과 같이 선택 확률을 결정한다.

$$
p_1 = b, \quad p_2 = 1 - b
$$

즉, $$ b $$ 값이 0.5이면 두 선택지를 균등한 확률로 선택하고, $$ b $$ 값이 1에 가까울수록 특정 선택지를 선호하게 된다. 이 모델은 단 하나의 자유 파라미터 $$ b $$ 만을 가진다

## Model 2: Noisy Win-Stay-Lose-Shift (노이즈가 추가된 승리-유지, 패배-전환 모델)

이 모델은 보상을 받으면 같은 선택을 반복하고, 보상을 받지 못하면 선택을 바꾸는 단순한 전략을 따른다. 그러나 이 전략을 항상 적용하는 것이 아니라 확률적으로 적용하는데, 확률 $$ 1 - \epsilon $$ 로 win-stay-lose-shift 규칙을 따르고, 확률 $$ \epsilon $$ 로 랜덤 선택을 한다. 이 모델에서 선택 확률은 다음과 같다.

$$
p_k^t =
\begin{cases}
1 - \frac{\epsilon}{2}, & \text{if } (c_{t-1} = k \text{ and } r_{t-1} = 1) \text{ OR } (c_{t-1} \neq k \text{ and } r_{t-1} = 0) \\
\frac{\epsilon}{2}, & \text{if } (c_{t-1} \neq k \text{ and } r_{t-1} = 1) \text{ OR } (c_{t-1} = k \text{ and } r_{t-1} = 0)
\end{cases}
$$

이 모델은 전체적인 랜덤성 수준을 조절하는 단 하나의 자유 파라미터 $$ \epsilon $$ 을 가진다.

## Model 3: Rescorla-Wagner (레스콜라-와그너 학습 모델)

이 모델은 참가자가 이전 결과를 바탕으로 각 슬롯 머신(옵션)의 기대 가치를 학습한다고 가정한다. 학습 규칙은 Rescorla-Wagner 학습 규칙을 따른다.

$$
Q_k(t+1) = Q_k(t) + \alpha (r_t - Q_k(t))
$$

여기서,

$$ Q_k(t) $$ 는 시간 $$ t $$ 에서의 선택지 $$ k $$ 의 기대 가치,  
$$ r_t $$ 는 시간 $$ t $$ 에서 받은 보상,  
$$ \alpha $$ 는 학습률로, 0과 1 사이의 값을 가지며, 보상이 가치 업데이트에 미치는 영향을 조절한다.  

의사결정 과정은 Softmax 선택 규칙을 따른다.

$$
p_k^t = \frac{\exp(\beta Q_k^t)}{\sum_{i=1}^{K} \exp(\beta Q_i^t)}
$$

여기서, $$ \beta $$ 는 ‘inverse temperature’로, 값이 크면 높은 가치를 가진 옵션을 더 확실하게 선택하고, 값이 작으면 랜덤한 선택을 많이 하게 된다. 이 모델은 두 개의 자유 파라미터 $$ (\alpha, \beta) $$ 를 가진다.

## Model 4: Choice Kernel (선택 경향성 모델)

이 모델은 사람들이 단순히 기대 가치를 기반으로 선택하는 것이 아니라, 과거에 선택했던 행동을 반복하는 경향이 있음을 반영한다. 이를 위해 Choice Kernel이라는 개념을 도입한다. Choice Kernel은 최근 선택했던 행동의 빈도를 추적하며, 다음과 같은 업데이트 방식을 따른다.

$$
CK_k(t+1) = CK_k(t) + \alpha_c (a_k(t) - CK_k(t))
$$

여기서, $$ a_k(t) = 1 $$ 이면 선택한 옵션이고, 그렇지 않으면 0이다. $$ \alpha_c $$ 는 선택 경향성을 업데이트하는 학습률이다. 이후 선택 확률은 Softmax 형태로 결정된다.

$$
p_k = \frac{\exp(\beta_c CK_k)}{\sum_{i=1}^{K} \exp(\beta_c CK_i)}
$$

이 모델은 두 개의 자유 파라미터 $$ (\alpha_c, \beta_c) $$ 를 가진다.

## Model 5: Rescorla-Wagner + Choice Kernel (강화 학습 + 선택 경향성)

이 모델은 강화 학습 모델과 선택 경향성 모델을 결합한 가장 복잡한 모델이다. 이 모델에서는 기대 가치를 업데이트하는 Rescorla-Wagner 모델과 과거 선택 경향성을 반영하는 Choice Kernel 모델이 함께 사용된다.

최종적으로 선택 확률은 다음과 같이 계산된다.

$$
p_k = \frac{\exp(\beta Q_k + \beta_c CK_k)}{\sum_{i=1}^{K} \exp(\beta Q_i + \beta_c CK_i)}
$$

즉, 선택 확률이 기대 가치 $$ Q_k $$ 와 선택 경향성 $$ CK_k $$ 의 합에 기반하여 결정된다. 이 모델은 총 네 개의 자유 파라미터 $$ (\alpha, \beta, \alpha_c, \beta_c) $$ 를 가진다.

이 다섯 개 모델은 참가자가 Multi-Armed Bandit Task에서 어떻게 선택을 하는지를 수학적으로 설명하기 위한 것이다. 모델의 복잡도는 점점 증가하며, 가장 간단한 모델은 Random Responding이고, 가장 복잡한 모델은 Rescorla-Wagner + Choice Kernel 모델이다.

- Random Responding: 무작위 선택 (자유 파라미터: $$ b $$)
- Noisy Win-Stay-Lose-Shift: 보상에 따라 반복 또는 변경 (자유 파라미터: $$ \epsilon $$)
- Rescorla-Wagner: 강화 학습 (자유 파라미터: $$ \alpha, \beta $$)
- Choice Kernel: 과거 선택 반복 경향 반영 (자유 파라미터: $$ \alpha_c, \beta_c $$)
- Rescorla-Wagner + Choice Kernel: 강화 학습 + 선택 경향 (자유 파라미터: $$ \alpha, \beta, \alpha_c, \beta_c $$)

각 모델은 인간의 학습 및 의사결정 과정을 수량화할 수 있도록 하며, 연구자들은 실험 데이터를 이 모델에 적합시켜 인간 행동의 기저 메커니즘을 분석할 수 있다.

<figure class='align-center'>
    <img src = "image path" alt="">
    <figcaption>figure 1. caption</figcaption>
</figure>
