---
title: "Review for Reinforcement Learning"
date: 2025-03-28
tags:
    - Reinforcement Learning
categories: 
    - Summary
toc: true
toc_sticky:  true
---

í˜„ì¬ê¹Œì§€ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ë§¤ìš° ë‹¤ì–‘í•œ ë°©ë©´ìœ¼ë¡œ ë°œì „í•´ ì™”ë‹¤. í•˜ì§€ë§Œ ê°•í™”í•™ìŠµì„ ì²˜ìŒ ê³µë¶€í•˜ëŠ” ì‚¬ëŒì˜ ì…ì¥ì—ì„œëŠ” ì´ëŸ° ì•Œê³ ë¦¬ì¦˜ì´ ì–´ë–»ê²Œ ì–½í˜€ìˆëŠ”ì§€ ê°ì„ ì¡ê¸° ì‰½ì§€ ì•Šë‹¤. ê·¸ë˜ì„œ ê°•í™”í•™ìŠµê³¼ ê´€ë ¨ëœ ë…¼ë¬¸ì„ ì½ì„ ë•Œ ì´ ì•Œê³ ë¦¬ì¦˜ì´ ì–´ë– í•œ ë°©ë²•ë¡ ì„ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì¸ì§€ ìŠ¤ìŠ¤ë¡œ ê°ì„ ì‰½ê²Œ ì¡ê¸° ìœ„í•´ì„œ ì •ë¦¬ í˜ì´ì§€ë¥¼ ë§Œë“¤ì—ˆë‹¤. ê°•í™”í•™ìŠµì˜ ì•„ì£¼ ê¸°ë³¸ì ì¸ ë‚´ìš©ì´ë‚˜ ì„¸ì„¸í•œ ìˆ˜í•™ì ì¸ ê°œë…ê¹Œì§€ ìì„¸í•˜ê²Œ ë‹¤ë£¨ì§€ëŠ” ì•Šê² ì§€ë§Œ í—·ê°ˆë¦¬ëŠ” ê°œë…ë“¤ ìœ„ì£¼ë¡œ ì•Œì•„ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤.

<br>

# 1. ì–´ë””ì— ì¤‘ì ì„ ë‘ëƒì— ëŒ€í•œ êµ¬ë¶„: Policy based approach / Value based approach

ê°•í™”í•™ìŠµì˜ ëª©í‘œëŠ” í•­ìƒ ë‹¤ìŒì˜ ê¸°ëŒ“ê°’ì„ ìµœëŒ€í™”í•˜ëŠ” ì •ì±… $\pi$ë¥¼ ì°¾ëŠ” ê²ƒì´ì—ˆë‹¤.

$$
J(\pi) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
$$

ì´ëŸ¬í•œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ì¢…ë¥˜ì˜ ì ‘ê·¼ ë°©ë²•ì„ ìƒê°í•  ìˆ˜ ìˆë‹¤.(ë¬¼ë¡  ë‚˜ì¤‘ì— ë” ë°œì „ëœ ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” ë‘ ë°©ì‹ì´ í˜¼í•©ëœ ë“¯í•œ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.) ì²« ë²ˆì§¸ëŠ” **policyë¥¼ ì§ì ‘ ëª¨ë¸ë§í•˜ì§€ ì•Šê³ , ìƒíƒœ ë˜ëŠ” ìƒíƒœ-í–‰ë™ ìŒì˜ ê°€ì¹˜ë¥¼ ì¶”ì •í•´ì„œ í–‰ë™ì„ ì„ íƒ**í•˜ëŠ” ê²ƒì´ê³ , ë‘ ë²ˆì§¸ëŠ” **policyë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•´ì„œ policyë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸**í•˜ëŠ” ê²ƒì´ë‹¤. ì „ìê°€ í”íˆ ë§í•˜ëŠ” value iteration, í›„ìê°€ policy iterationì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.

## Value based approach - example.Q-learning

state value function VëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë˜ë©°

$$
V^\pi(s) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]
$$

state-action value QëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.

$$
Q^\pi(s, a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]
$$

ì—¬ê¸°ì„œ VëŠ” í˜„ì¬ ìƒíƒœì—ì„œ ì„ íƒí•  ìˆ˜ ìˆëŠ” ëª¨ë“  í–‰ë™ aì™€ ë‹¤ìŒ ìƒíƒœë“¤ì„ ëª¨ë‘ ê³ ë ¤í•´ì„œ í˜„ì¬ ìƒíƒœì˜ ê¸°ëŒ“ê°’ì„ í‘œí˜„í•˜ë©°, QëŠ” í˜„ì¬ ìƒíƒœì˜ **ì–´ë–¤ í–‰ë™ a**ë¥¼ ì„ íƒí–ˆì„ ë•Œ ê·¸ ì´í›„ë¥¼ ê³ ë ¤í•´ì„œ í˜„ì¬ ìƒíƒœ-í–‰ë™ì˜ ê¸°ëŒ“ê°’ì„ í‘œí˜„í•œë‹¤. ë”°ë¼ì„œ ì–´ë–¤ policyë¥¼ ë”°ë¥´ëŠ” Vì™€ Qì˜ ê´€ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$
V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot|s)} \left[ Q^\pi(s, a) \right]
$$

$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \cdot Q^\pi(s, a)
$$

ì´ëŸ¬í•œ valueì™€ ê´€ë ¨ëœ ê°’ì„ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì—ëŠ” ëŒ€í‘œì ìœ¼ë¡œ SARSA, DQNì´ ìˆë‹¤. ì´ëŸ¬í•œ value based approachì—ì„œëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ì§ê´€ì ì´ê³  ì•ˆì •ì ì´ë©°, action spaceê°€ discreteí•œ ê²½ìš° ë§¤ìš° ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ì§€ë§Œ continuous action spaceì—ì„œëŠ” ë™ì‘ì´ ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë˜í•œ **policy $\pi$**ì— ê°„ì ‘ì ìœ¼ë¡œ ì ‘ê·¼í•œë‹¤ëŠ” ì  ë˜í•œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆë‹¤. ì–´ë””ê¹Œì§€ë‚˜ ì´ëŸ¬í•œ ë°©ë²•ë¡ ì€ ì ì°¨ í–‰ë™ì„ ê°œì„ ì‹œì¼œê°€ë©° ì •ì±…ì„ implicití•˜ê²Œ ë°œì „ì‹œí‚¤ê¸° ë•Œë¬¸ì´ë‹¤.

## Policy based approach - example.REINFORCE

ê°•í™”í•™ìŠµì˜ ëª©í‘œ í•¨ìˆ˜ë¥¼ í•œ ë²ˆ ë” ë‹¤ì‹œ ë³´ì.

$$
J(\pi) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
$$

ì´ë²ˆì—ëŠ” policy $\pi_\theta(a \mid s)$ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§ì ‘ íŒŒë¼ë¯¸í„°í™” í•´ì„œ í•™ìŠµì„ í•´ ë³¼ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ value functionì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠëƒ? í•œë‹¤ë©´ ë‹µì€ **ê·¸ëŸ´ ìˆ˜ë„ ìˆê³  ì•„ë‹ ìˆ˜ë„ ìˆë‹¤.** ì‚¬ì‹¤ ìµœê·¼ ë°œì „ëœ ê¸°ë²•ë“¤ì€ **Actor-Critic** êµ¬ì¡°ë¼ê³  í•´ì„œ ëŒ€ë¶€ë¶„ ì´ valueì™€ policyë¥¼ ë™ì‹œì— ì‚¬ìš©í•œë‹¤. ì´ ê¸°ë²•ë“¤ì€ ë’¤ì—ì„œ ë‹¤ë£¨ê¸°ë¡œ í•˜ê³ , ì—¬ê¸°ì„œëŠ” policyë§Œ ëª…ì‹œì ìœ¼ë¡œ ë°œì „ì‹œí‚¤ëŠ” ê²½ìš°ì— í•œí•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ì.

ëª…ì‹œì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ ìˆëŠ” policyë¥¼ ë³€í™”ì‹œí‚¤ë ¤ë©´ policyì— ëŒ€í•œ gradientê°€ í•„ìš”í•  ê²ƒì´ë¼ê³  ì§ì‘í•  ìˆ˜ ìˆë‹¤. ì´ê²ƒì„ í•œ ë²ˆ êµ¬í•´ë³¼ ê²ƒì´ë‹¤. ìš°ì„  trajectoryì˜ í™•ë¥ ì„ ìƒê°í•´ ë³´ì. ì •ì±…ì— ë”°ë¼ ì–´ë–¤ trajectory Ï„ê°€ ë‚˜ì˜¬ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ $\pi$ëŠ” ìƒíƒœë¥¼ ë°›ì•„ í–‰ë™ìœ¼ë¡œ ì¹˜í™˜í•˜ëŠ” ì–´ë– í•œ ë¶„í¬, $\rho$ëŠ” ì´ˆê¸° ìƒíƒœê°€ ì„ íƒë  í™•ë¥ ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.

$$
P(\tau) = \rho(s_0) \prod_{t=0}^{T} \pi_\theta(a_t | s_t) \cdot p(s_{t+1} | s_t, a_t)
$$

ë”°ë¼ì„œ ê¸°ëŒ€ returnì„ trajectoryì— ëŒ€í•œ ì ë¶„ í˜•íƒœë¡œ ì“°ë©´ ë‹¤ìŒì²˜ëŸ¼ ì“¸ ìˆ˜ ìˆë‹¤.

$$
J(\theta) = \int_\tau P(\tau) R(\tau) d\tau
$$

ê¸°ëŒ€ê°’ì˜ gradientëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¯¸ë¶„í•  ìˆ˜ ìˆë‹¤.

$$
\nabla_\theta J(\theta)
= \nabla_\theta \int_\tau P(\tau) R(\tau) d\tau
= \int_\tau \nabla_\theta P(\tau) R(\tau) d\tau
$$

ì—¬ê¸°ì„œ âˆ‡Î¸P(Ï„)ì— log-derivative trickì„ ì ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤. (ì´ëŠ” ì¦ëª…ì´ ë˜ì–´ ìˆê³ , ìì„¸í•œ ì¦ëª… ê³¼ì •ì€ ìƒëµí•œë‹¤.)

$$
\nabla_\theta P(\tau) = P(\tau) \cdot \nabla_\theta \log P(\tau)
$$

í™˜ê²½ì˜ dynamicsëŠ” ì •í™•íˆ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ê²½ë¡œì ë¶„ê³¼ í•©ì³ì„œ estimationì— ëŒ€í•œ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$
\nabla_\theta J(\theta)
= \int_\tau P(\tau) \nabla_\theta \log P(\tau) R(\tau) d\tau
= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log P(\tau) \cdot R(\tau) \right]
$$

í™˜ê²½ì˜ dynamics ğ‘(ğ‘ â€²âˆ£ğ‘ ,ğ‘)ëŠ” ğœƒì™€ ë¬´ê´€í•˜ê¸° ë•Œë¬¸ì— ë¯¸ë¶„í•  í•„ìš”ê°€ ì—†ë‹¤. ë”°ë¼ì„œ logğ‘ƒ(ğœ)ì—ì„œ ì •ì±… ë¶€ë¶„ë§Œ ë‚¨ëŠ”ë‹¤.

$$
\log P(\tau) = \log \left( \rho(s_0) \prod_{t=0}^{T} \pi_\theta(a_t | s_t) \cdot p(s_{t+1} | s_t, a_t) \right)
= \sum_{t=0}^{T} \log \pi_\theta(a_t | s_t) + \text{(ì •ì±…ê³¼ ë¬´ê´€í•œ ë¶€ë¶„)}
$$

ë”°ë¼ì„œ ë¯¸ë¶„í•´ë„ ì •ì±… ë¶€ë¶„ë§Œ ë‚¨ëŠ”ë‹¤.

$$
\nabla_\theta \log P(\tau) = \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t)
$$

ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.

$$
\nabla_\theta J(\theta)
= \mathbb{E}_{\tau \sim \pi_\theta} \left[
    \left( \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) \right)
    \cdot R(\tau)
\right]
$$

ì´ì œ ë³´ìƒ Rì„ ì‹œê°„ të§ˆë‹¤ ë‚˜ëˆ ì„œ ì“°ë©´:

$$
\nabla_\theta J(\theta)
= \mathbb{E}_{\tau \sim \pi_\theta} \left[
    \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot G_t
\right]
$$

ì‚¬ì‹¤ ì›ë˜ëŒ€ë¡œë¼ë©´ Q ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë§ë‹¤. 

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot Q^\pi(s,a) \right]
$$

í•˜ì§€ë§Œ policy based approachì—ì„œëŠ” policyë¥¼ **í™•ë¥ ë¶„í¬ì— ê·¼ì‚¬í•˜ì—¬ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— Q functionì˜ ê°’ì´ë‚˜ Value functionì„ ì •í™•í•˜ê²Œ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ì¹˜ëª…ì ì¸ ë‹¨ì ì´ ì¡´ì¬í•œë‹¤.** ì¦‰ ì—…ë°ì´íŠ¸ ì‹œ Qê°’ì„ ì¶”ì‚°í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— episodeê°€ ëë‚œ í›„ì¸ ë³´ìƒ Gë¡œ ëŒ€ì²´í•˜ì—¬ ì—…ë°ì´íŠ¸í•œë‹¤. ì´ë ‡ê²Œ episodeê°€ ëë‚  ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì‹ì„ **Monte-Carlo**ë°©ì‹ì´ë¼ê³  í•˜ë©°, Policy based approachë¥¼ ì·¨í•˜ë©° MC ë°©ì‹ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì´ëŸ° ë°©ë²•ë¡ ì„ ì·¨í•œ ì•Œê³ ë¦¬ì¦˜ì´ ë°”ë¡œ **REINFORCE** ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ì ì€ ì—¬ê¸°ì„œ íŒŒìƒí•´ì„œ **MC ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šì„ ìˆ˜ë„, ë‹¨ìˆœíˆ policy based approachë§Œì„ ì·¨í•˜ì§€ ì•Šì„ ìˆ˜ë„** ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

REINFORCEì˜ íë¦„ë„ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Initialize policy network Ï€(a|s; Î¸)                                       â”‚
â”‚    - Outputs probability distribution over actions for a given state s       â”‚
â”‚    - Example output: logits â†’ softmax â†’ Ï€(a|s)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. For each episode:                                                         â”‚
â”‚    - Reset environment â†’ receive initial state sâ‚€                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. For each time step t in the episode:                                     â”‚
â”‚   1) Sample action from policy:                                              â”‚
â”‚        aâ‚œ ~ Ï€(a|sâ‚œ; Î¸)   â† stochastic sampling                               â”‚
â”‚   2) Execute action aâ‚œ in the environment                                    â”‚
â”‚        â†’ Observe reward râ‚œâ‚Šâ‚ and next state sâ‚œâ‚Šâ‚                             â”‚
â”‚   3) Store (sâ‚œ, aâ‚œ, râ‚œâ‚Šâ‚) into trajectory buffer                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. After episode ends (trajectory complete):                                 â”‚
â”‚   For each time step t = 0 to T:                                             â”‚
â”‚   1) Compute return Gâ‚œ from t onward:                                        â”‚
â”‚        Gâ‚œ = âˆ‘_{k=0}^{Tâˆ’tâˆ’1} Î³^k Â· r_{t+k+1}                                   â”‚
â”‚        â†’ Total discounted future reward from step t                          â”‚
â”‚   2) Compute policy loss:                                                    â”‚
â”‚        Lâ‚œ = -log Ï€(aâ‚œ|sâ‚œ; Î¸) Â· Gâ‚œ                                           â”‚
â”‚        â†’ Encourages high-return actions by increasing their log-prob         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Backpropagate total loss and update policy network:                      â”‚
â”‚     Total Loss = âˆ‘_t Lâ‚œ                                                      â”‚
â”‚     Î¸ â† Î¸ - Î± âˆ‡Î¸ Total Loss   â† Gradient ascent step                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Repeat for next episode                                                  â”‚
â”‚     â†’ Continue collecting trajectories and updating policy                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## íŠ¹ì´ì 

ì—¬ê¸°ì„œë¶€í„°ëŠ” ìœ„ì˜ ì´ë¶„ë²•ì ì¸ êµ¬ì¡°ì—ì„œ ë²—ì–´ë‚˜ ë‘˜ì„ ìœµí•©í•˜ê¸°ë„, ë˜ëŠ” ìƒˆë¡œìš´ ê°œë…ì„ ë„ì…í•˜ì—¬ í•™ìŠµì„ ë” robustí•˜ê²Œ ì¼ì–´ë‚˜ê²Œ í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³¼ ê²ƒì´ë‹¤. ê·¸ ë’¤ì˜ PPO, RLHF ë“±ì˜ í˜„ì¬ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ ë°‘ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ë°œì „ì‹œí‚¨ ê²ƒì´ë‹¤.

### target networkì˜ ì¶”ê°€ - example.Dueling DQN

dueling DQNì€ DQNì—ì„œ ë‘ ê°€ì§€ê°€ ë°”ë€ DQNì´ë‹¤. ì¦‰ ë°”ë€ ë¶€ë¶„ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ê¸°ì¡´ DQNê³¼ ì™„ì „íˆ ë™ì¼í•˜ë‹¤. ì—¬ê¸°ì„œëŠ” vanila DQNì— ëŒ€í•´ì„œ ì´í•´í•˜ê³  ìˆë‹¤ëŠ” ê°€ì • í•˜ì— ì„¤ëª…ì„ ì§„í–‰í•œë‹¤.

í•µì‹¬ ì•„ì´ë””ì–´ëŠ” Q(s, a) ë¥¼ ìƒíƒœì˜ valueì™€ í–‰ë™ì˜ advantageë¡œ ë‚˜ëˆ ì„œ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤. (ì—¬ê¸°ì„œì˜ advantageëŠ” ë’¤ì—ì„œ ë‚˜ì˜¬ trpoì˜ advantageì™€ëŠ” ê°œë…ë„, ì—­í• ë„ ë‹¤ë¦„ì— ìœ ì˜)

$$
Q(s, a) = V(s) + \left( A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a') \right)
$$

ì—¬ê¸°ì„œ VëŠ” ìƒíƒœì˜ ì ˆëŒ€ì ì¸ ê°€ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, AëŠ” **íŠ¹ì • í–‰ë™ì´ ë‹¤ë¥¸ í–‰ë™ë“¤ì— ë¹„í•´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.** ì—¬ê¸°ì„œì˜ advantageì˜ ì—­í• ì€ Q valueë¥¼ ê³„ì‚°í•˜ëŠ” ë° ìˆì–´ì„œ ì•ˆì •ì„±ì„ ë¶€ì—¬í•˜ê¸° ìœ„í•œ ì¥ì¹˜ì´ë‹¤. ì´ë¥¼ ê³„ì‚°í•  ë•Œì—ëŠ” ë‘˜ ë‹¤ ë™ì¼í•˜ê²Œ CNN/MLP layerë¥¼ ê³µìœ í•˜ë©° ì´í›„ ë‘ ê°ˆë˜ë¡œ ë‚˜ë‰˜ì–´ì„œ ê°ê° Vë¥¼ ë°˜í™˜í•˜ëŠ” value head, advantageë¥¼ ë°˜í™˜í•˜ëŠ” advantage headë¥¼ í†µí•´ ê°’ì´ ì¶œë ¥ëœë‹¤. ì´ë¥¼ íë¦„ë„ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Initialize main Q-network Î¸                 â”‚
â”‚ - Shared layers                             â”‚
â”‚ - Value stream:      V(s; Î¸_V)              â”‚
â”‚ - Advantage stream:  A(s, a; Î¸_A)           â”‚
â”‚ Combine: Q(s,a) = V(s) + [A(s,a) - mean(A)] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Initialize target network Î¸â» â† Î¸            â”‚
â”‚ (for stable target computation)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each episode:                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each time step t:                       â”‚
â”‚ 1. Observe current state sâ‚œ                  â”‚
â”‚ 2. Compute Q(sâ‚œ, a; Î¸) for all a             â”‚ â† âœ… Policy Evaluation (Value Estimation)
â”‚ 3. Choose action aâ‚œ ~ Îµ-greedy(Q(sâ‚œ, a))      â”‚ â† ğŸ¯ Policy Improvement (implicit via Îµ-greedy)
â”‚ 4. Take action aâ‚œ, observe râ‚œâ‚Šâ‚, sâ‚œâ‚Šâ‚         â”‚
â”‚ 5. Store (sâ‚œ, aâ‚œ, râ‚œâ‚Šâ‚, sâ‚œâ‚Šâ‚) in buffer        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample random mini-batch from replay buffer â”‚  â† for desired td update step(normally each time step ends;)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each sample (s, a, r, s'):              â”‚
â”‚ 1. Compute target:                          â”‚
â”‚    y = r + Î³Â·max_{a'} Q(s', a'; Î¸â»)         â”‚ â† âœ… TD Target (Value Improvement)
â”‚ 2. Compute predicted Q(s, a; Î¸)             â”‚
â”‚ 3. Compute loss:                            â”‚
â”‚    L = (Q(s, a; Î¸) - y)Â²                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backpropagate loss, update Î¸                â”‚ â† âœ… Gradient Step (Value Network Improvement)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Periodically update target network Î¸â» â† Î¸   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ê¸°ì¡´ DQNê³¼ ë‹¤ë¥¸ ì°¨ì´ì ì€ target networkë¥¼ ë”°ë¡œ ë‘ì–´ì„œ ì¼ì •í•œ ì£¼ê¸°ë¡œ main networkë¡œ ë®ì–´ì”Œìš°ëŠ” ê±°ì´ë‹¤. ì´ëŠ” ê¸°ì¡´ DQNì—ì„œ ë§¤ stepë§ˆë‹¤ networkì„ freezeí•´ì„œ ì—…ë°ì´íŠ¸ì˜ ê¸°ì¤€ìœ¼ë¡œ ë‘ì—ˆë˜ ê²ƒê³¼ ë¹„êµí•´ì„œ ì•ˆì •ì„±ì´ ì¡°ê¸ˆ ë” í™•ë³´ë˜ëŠ” ì¥ì ì´ ìˆë‹¤.

### Deterministic Policy Gradient, actor-critic structure - example.DDPG

ë§¨ ì²˜ìŒ ì‚´í´ë³´ì•˜ë˜ REINFORCEì—ì„œëŠ” policyê°€ stochasticí•œ í™•ë¥ ë¶„í¬ë¡œì¨ ì´ë£¨ì–´ì¡Œë‹¤. ë°˜ë©´ ì—¬ê¸°ì„œëŠ” policyì— ë”°ë¼ actionì´ ê³ ì •ë˜ì–´ ìˆëŠ” **deterministic policy**ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì— ëŒ€í•´ ì•Œì•„ë³¼ ê²ƒì´ë‹¤. ì‚¬ì‹¤ ì´ ëª¨ë¸ì€ DQNì„ ì—°ì† ì œì–´ì—ì„œ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ì•„ì´ë””ì–´ì—ì„œ ì¶œë°œí•œ ëª¨ë¸ì´ë‹¤.

DDPGëŠ” ì´ì „ì— Dueling DQNì—ì„œ ë³´ì•˜ë˜ target networkë¥¼ ì±„ìš©í•¨ê³¼ ë™ì‹œì—, actor critic êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ì´ë‹¤. ë„¤íŠ¸ì›Œí¬ëŠ” ë‘ ê°œì˜ ë©”ì¸ ë„¤íŠ¸ì›Œí¬ì¸ actor network(ìƒíƒœ së¥¼ ë°›ì•„ í–‰ë™ aë¥¼ ë°˜í™˜), critic network(ìƒíƒœ s, í–‰ë™ aë¥¼ ë°›ì•„ ë³´ìƒ ë˜ëŠ” ê°€ì¹˜ë¥¼ ë°˜í™˜)ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©°, ì´ë“¤ì˜ target ì—­í• ì„ í•˜ëŠ” target actor, target critic networkê¹Œì§€ í•©í•´ ì´ 4ê°œì˜ ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ëœë‹¤.

DDPGê°€ í–‰ë™ì„ ì‹¤í–‰í•  ë•Œì—ëŠ” policyë§ˆë‹¤ actionì€ deterministic í•˜ì§€ë§Œ ì´ ë•Œ noiseë¥¼ ì²¨ê°€í•´ì„œ ì–´ëŠ ì •ë„ varianceë¥¼ ë³´ì¥í•œë‹¤. replay bufferì˜ ì•„ì´ë””ì–´ëŠ” DQNì—ì„œ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™”ìœ¼ë‹ˆ ìƒëµí•˜ê³ , ê·¸ ì´í›„ ê³¼ì •ì„ ì•Œì•„ë³´ì. 

replay bufferì—ëŠ” $(s_t, a_t, r_{t+1}, s_{t+1})$ì´ ì €ì¥ë˜ì–´ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ targetì„ ê³„ì‚°í•˜ëŠ” ë° ìˆì–´ì„œ $\text{td target y} = r + \gamma * Q^{'}(s', a')$ì´ë¯€ë¡œ s', rì€ ìˆìœ¼ë¯€ë¡œ ìƒíƒœ së¥¼ **target actor networkì— ì§‘ì–´ë„£ì–´** ë‹¤ìŒ í–‰ë™ a'ì„ ë°˜í™˜(ì˜ˆì¸¡)í•˜ë„ë¡ í•œ ë’¤, ì´ (s', a') ìŒì„ **target critic network**ì— ì§‘ì–´ë„£ì–´ Q'ê°’ì„ ì•Œ ìˆ˜ ìˆë‹¤. 

ê·¸ë¦¬ê³  (s, a) ìŒì„ ì´ìš©í•´ì„œ **main actor network, main critic network**ì— í†µê³¼ì‹œì¼œ ì˜ˆì¸¡ Qê°’ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ ë‘ Q, Q'ê°’ì˜ ì°¨ì´ë¥¼ ì´ìš©í•´ lossë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. 

$$
\begin{aligned}
&\text{Target:} \quad y = r + \gamma \cdot Q'(s', \mu'(s')) \\\\
&\text{Loss:} \quad L = \left( Q(s, a; \theta^Q) - y \right)^2
\end{aligned}
$$
ì´ loss ë¥¼ ì´ìš©í•´ critic networkë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ ,

$$
\theta^Q \leftarrow \theta^Q - \alpha_Q \nabla_{\theta^Q} L
$$

$$
\nabla_{\theta^Q} L = \mathbb{E}_{(s,a)} \left[
  2 \cdot \left( Q(s,a;\theta^Q) - y \right) \cdot \nabla_{\theta^Q} Q(s,a;\theta^Q)
\right]
$$

ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ actor lossë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì¡°ê¸ˆ ë” ë³µì¡í•œë°, ì¼ë‹¨ main actor networkì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$
\theta^\mu \leftarrow \theta^\mu + \alpha_\mu \nabla_{\theta^\mu} J
$$

ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ ê²ƒì€:

$$
\nabla_{\theta^\mu} J = \nabla_{\theta^\mu} Q(s, \mu(s))
$$

ê·¸ëŸ°ë° ì´ QëŠ” aì— ëŒ€í•´ ì •ì˜ëœ í•¨ìˆ˜ì´ë¯€ë¡œ, chain ruleì„ ì¨ì•¼ í•œë‹¤. DDPGì—ì„œ actorì˜ ëª©ì ì€ 

$$
J(\theta^\mu) = \mathbb{E}_{s \sim D} \left[ Q(s, \mu(s; \theta^\mu)) \right]
$$

ì¦‰, actorëŠ” ìì‹ ì˜ ì •ì±…ì´ ë§Œë“¤ì–´ë‚´ëŠ” í–‰ë™ì´ ë†’ì€ Qê°’ì„ ê°€ì§€ê²Œ í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤. ì´ì œ ëª©ì í•¨ìˆ˜ Jë¥¼ íŒŒë¼ë¯¸í„° ğœƒğœ‡ì— ëŒ€í•´ ë¯¸ë¶„í•œë‹¤.

$$
\nabla_{\theta^\mu} J(\theta^\mu)
= \nabla_{\theta^\mu} \mathbb{E}_{s \sim D} \left[ Q(s, \mu(s; \theta^\mu)) \right]
$$

$$
= \mathbb{E}_{s \sim D} \left[ \nabla_{\theta^\mu} Q(s, \mu(s)) \right]
$$

Q(s, a)ëŠ” aì— ëŒ€í•œ í•¨ìˆ˜ì´ê³ , aëŠ” $\mu(s; \theta^{\mu})$ë¡œ ì •ì˜ë˜ëŠ” í•¨ìˆ˜ì´ë¯€ë¡œ í•©ì„±í•¨ìˆ˜ì´ë‹¤. ì´ì œ chain ruleì„ ì‚¬ìš©í•˜ë©´

$$
\nabla_{\theta^\mu} Q(s, \mu(s))
= \nabla_a Q(s,a) \Big|_{a = \mu(s)} \cdot \nabla_{\theta^\mu} \mu(s)
$$

ë”°ë¼ì„œ ì „ì²´ gradientëŠ”:

$$
\nabla_{\theta^\mu} J(\theta^\mu)
= \mathbb{E}_{s \sim D} \left[
  \nabla_a Q(s,a) \Big|_{a = \mu(s)} \cdot \nabla_{\theta^\mu} \mu(s)
\right]
$$

ì´ëŸ¬í•œ ê³¼ì •ì´ ëë‚˜ë©´ ì¼ì • ì£¼ê¸°ë§ˆë‹¤ main networkë¥¼ target networkë¡œ ë³µì‚¬í•˜ëŠ” soft target updateë¥¼ í†µí•´ í•™ìŠµ ì•ˆì •ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”                                                           â”‚
â”‚   - Actor network Î¼(s; Î¸^Î¼): Deterministic Policy                            â”‚
â”‚   - Critic network Q(s,a; Î¸^Q): Action-value function                        â”‚
â”‚   - Target networks: Î¼â€², Qâ€² (soft target updatesìš©)                           â”‚
â”‚     Î¸^{Î¼'} â† Î¸^Î¼,   Î¸^{Q'} â† Î¸^Q                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Replay buffer D ì´ˆê¸°í™”                                                   â”‚
â”‚   - ê²½í—˜ì„ ì €ì¥í•˜ê³  ìƒ˜í”Œë§í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. For each episode:                                                        â”‚
â”‚   - í™˜ê²½ ì´ˆê¸°í™” â†’ ì´ˆê¸° ìƒíƒœ sâ‚€                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. For each time step t in episode:                                         â”‚
â”‚   1) í–‰ë™ ì„ íƒ:                                                             â”‚
â”‚       aâ‚œ = Î¼(sâ‚œ; Î¸^Î¼) + noise                                               â”‚
â”‚       â†’ Exploration ìœ„í•´ OU noise ë“± ì‚¬ìš© ê°€ëŠ¥                             â”‚
â”‚   2) í–‰ë™ ì‹¤í–‰ â†’ ë³´ìƒ râ‚œâ‚Šâ‚, ë‹¤ìŒ ìƒíƒœ sâ‚œâ‚Šâ‚ ê´€ì¸¡                             â”‚
â”‚   3) (sâ‚œ, aâ‚œ, râ‚œâ‚Šâ‚, sâ‚œâ‚Šâ‚) â†’ replay buffer D ì €ì¥                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Mini-batch ìƒ˜í”Œë§                                                        â”‚
â”‚   - Dì—ì„œ ë¬´ì‘ìœ„ transition ìƒ˜í”Œ Kê°œ ì¶”ì¶œ                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. For each sample (s, a, r, s'):                                           â”‚
â”‚   1) Target action ê³„ì‚°: a' = Î¼â€²(s')                                         â”‚
â”‚   2) Target Q ê³„ì‚°:                                                         â”‚
â”‚        y = r + Î³ Â· Qâ€²(s', a')             â† âœ… TD Target                     â”‚
â”‚   3) Critic loss ê³„ì‚°:                                                      â”‚
â”‚        L = (Q(s,a; Î¸^Q) - y)Â²             â† TD error                         â”‚
â”‚   4) Critic ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸:                                              â”‚
â”‚        Î¸^Q â† Î¸^Q - Î±_Q âˆ‡_Î¸^Q L            â† âœ… Value Learning                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Actor ì—…ë°ì´íŠ¸ (Policy Improvement):                                     â”‚
â”‚   1) Criticì„ í†µí•´ gradient ì¶”ì¶œ:                                           â”‚
â”‚        âˆ‡_Î¸^Î¼ J â‰ˆ âˆ‡_a Q(s,a) Â· âˆ‡_Î¸^Î¼ Î¼(s)                                     â”‚
â”‚   2) Actor ì—…ë°ì´íŠ¸:                                                        â”‚
â”‚        Î¸^Î¼ â† Î¸^Î¼ + Î±_Î¼ âˆ‡_Î¸^Î¼ J                                               â”‚
â”‚      â†’ deterministic policyë¥¼ Q-valueê°€ ë†’ì€ ë°©í–¥ìœ¼ë¡œ ê°œì„                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Soft target network ì—…ë°ì´íŠ¸ (Polyak averaging)                          â”‚
â”‚     Î¸^{Î¼'} â† Ï„ Î¸^Î¼ + (1 - Ï„) Î¸^{Î¼'}                                          â”‚
â”‚     Î¸^{Q'} â† Ï„ Î¸^Q + (1 - Ï„) Î¸^{Q'}                                          â”‚
â”‚     â†’ ì²œì²œíˆ ë”°ë¼ê°€ëŠ” targetì´ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
|------------|------|
| **Actor** | ìƒíƒœ â†’ í–‰ë™ (Deterministic policy Î¼(s)) |
| **Critic** | Q(s,a) ì¶”ì •. Actorì˜ ì„±ëŠ¥ í”¼ë“œë°± ì œê³µ |
| **Replay buffer** | Off-policy í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í•¨ |
| **Target networks** | ì•ˆì •ì  TD target ìƒì„±ì— í•„ìˆ˜ |
| **TD target** | `y = r + Î³Â·Q'(s', Î¼'(s'))` (bootstrapping) |
| **Policy Gradient** | `âˆ‡_Î¸^Î¼ J â‰ˆ âˆ‡_a Q(s,a) Â· âˆ‡_Î¸^Î¼ Î¼(s)` |


### TRPO


<br>

# 2. ê°€ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ êµ¬ë¶„: MC / TD





<br>

